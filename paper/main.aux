\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Jadbabaie2015,SPM}
\citation{Asif2014,Akhriev2020,SPM}
\citation{Besbes2013,Jadbabaie2015,SPM,NaLi2020}
\citation{Jadbabaie2015}
\citation{Dontchev2013,simonetto_time-varying_2020}
\citation{Simonetto2014d,Bastianello2020asi}
\newlabel{sec:introduction}{{1}{1}{}{section.1}{}}
\citation{SPM,simonetto_time-varying_2020}
\citation{Jadbabaie2015}
\citation{Simonetto2014d,Bastianello2020asi}
\citation{Devolder2011,Nedic2011}
\citation{Nghiem2018,Ongie2020}
\citation{Seijo2011,Lim2012,Mazumder2019,Blanchet2019}
\citation{simonetto_smooth_2021}
\citation{Taylor2016,Taylor2017}
\citation{ryu_operator_2020}
\citation{valentine_lipschitz_1945}
\citation{pmlr-v119-mai20a}
\citation{Duchi2018,Davis2019,pmlr-v119-mai20b}
\newlabel{fig.1}{{1}{2}{The idea of boosting via projection onto the space of ``good'' functions or ``good'' fixed point operators. One can interpret the evaluation of function $f$ or operator $\mathcal {T}$ as noisy evaluations of an underlying ``better'' function or operator, $\hat {f}$ and $\hat {\mathcal {T}}$, respectively, and use the latter to solve the problem instead. This gives rise to convex-regression-based boosting or operation-regression-based boosting (OpReg-Boost)}{figure.1}{}}
\citation{Simonetto20XX}
\citation{Mazumder2019,simonetto_smooth_2021}
\newlabel{eq:continuous-time-problem}{{1}{3}{}{equation.2.1}{}}
\newlabel{eq:base-problem}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:fb}{{5}{3}{}{equation.2.5}{}}
\newlabel{eq.inf}{{6}{3}{}{equation.3.6}{}}
\citation{Rockafellar1976,Eckstein1989,bauschke_convex_2017,Ryu2015,Sherson2018}
\citation{linear}
\citation{ryu_operator_2020}
\citation{valentine_lipschitz_1945}
\citation{valentine_lipschitz_1945}
\citation{valentine_lipschitz_1945}
\newlabel{socp}{{7}{4}{}{equation.3.7}{}}
\newlabel{interp}{{8}{4}{}{equation.3.8}{}}
\newlabel{sec:opreg}{{4}{4}{}{section.4}{}}
\newlabel{eq:operator-regression}{{12}{4}{}{equation.4.12}{}}
\newlabel{eq:interpolation}{{13}{4}{}{equation.4.13}{}}
\citation{reich_projection_2015}
\citation{.}
\citation{.}
\newlabel{eq:lipschitz-interpolation}{{14}{5}{}{equation.4.14}{}}
\newlabel{eq:equivalent-problem}{{16}{5}{}{equation.4.16}{}}
\newlabel{eq:interpolation-constraints}{{16b}{5}{}{equation.4.2}{}}
\newlabel{eq:consensus-constraints}{{16c}{5}{}{equation.4.3}{}}
\newlabel{eq:prs-xi}{{17a}{5}{}{equation.4.1}{}}
\newlabel{eq:local-updates}{{18}{5}{}{equation.4.18}{}}
\newlabel{sec:online-opreg}{{5}{6}{}{section.5}{}}
\newlabel{sec:numerical}{{6}{6}{}{section.6}{}}
\citation{beck_fast_2009}
\citation{pmlr-v119-mai20a}
\citation{beck_fast_2009}
\citation{pmlr-v119-mai20a}
\citation{duchi_stochastic_2018}
\citation{drusvyatskiy_error_2018}
\citation{drusvyatskiy_error_2018}
\citation{drusvyatskiy_error_2018}
\newlabel{eq:linear-regression}{{20}{7}{}{equation.6.20}{}}
\newlabel{fig:lasso-operator_calls}{{2}{7}{Comparison of the cumulative tracking error evolution for forward-backward, its accelerated versions FISTA \cite {beck_fast_2009} and Anderson \cite {pmlr-v119-mai20a}, OpReg-Boost and interpolated OpReg-Boost, and CvxReg-Boost. The methods are given a budget of $10$ gradient calls per sampling time}{figure.2}{}}
\newlabel{eq:phase-retrieval}{{21}{7}{}{equation.6.21}{}}
\newlabel{fig:phase-retrieval}{{3}{7}{Comparison of the cumulative tracking error evolution for prox-linear \cite {drusvyatskiy_error_2018}, the Anderson acceleration version, and OpReg-Boost. The methods are given a budget of $10$ operator evaluations per sampling time}{figure.3}{}}
\bibdata{PaperCollection00,references}
\bibcite{Akhriev2020}{{1}{2020}{{Akhriev et~al.}}{{Akhriev, Marecek, and Simonetto}}}
\bibcite{artacho_new_2018}{{2}{2018}{{Artacho \& Campoy}}{{Artacho and Campoy}}}
\bibcite{Asif2014}{{3}{2014}{{Asif \& Romberg}}{{Asif and Romberg}}}
\bibcite{Bastianello2020asi}{{4}{2020}{{Bastianello et~al.}}{{Bastianello, Simonetto, and Carli}}}
\bibcite{reich_projection_2015}{{5}{2015}{{Bauschke \& Koch}}{{Bauschke and Koch}}}
\bibcite{bauschke_convex_2017}{{6}{2017}{{Bauschke \& Combettes}}{{Bauschke and Combettes}}}
\bibcite{bauschke_characterizing_2009}{{7}{2009}{{Bauschke et~al.}}{{Bauschke, Deutsch, and Hundal}}}
\bibcite{beck_fast_2009}{{8}{2009}{{Beck \& Teboulle}}{{Beck and Teboulle}}}
\bibcite{Besbes2013}{{9}{2015}{{Besbes et~al.}}{{Besbes, Gur, and Zeevi}}}
\bibcite{Blanchet2019}{{10}{2019}{{Blanchet et~al.}}{{Blanchet, Glynn, Yan, and Zhou}}}
\bibcite{SPM}{{11}{2020}{{Dall'Anese et~al.}}{{Dall'Anese, Simonetto, Becker, and Madden}}}
\bibcite{Davis2019}{{12}{2019}{{Davis \& Drusvyatskiy}}{{Davis and Drusvyatskiy}}}
\bibcite{Devolder2011}{{13}{2012}{{Devolder et~al.}}{{Devolder, Glineur, and Nesterov}}}
\bibcite{Dontchev2013}{{14}{2013}{{Dontchev et~al.}}{{Dontchev, Krastanov, Rockafellar, and Veliov}}}
\bibcite{drusvyatskiy_error_2018}{{15}{2018}{{Drusvyatskiy \& Lewis}}{{Drusvyatskiy and Lewis}}}
\bibcite{Duchi2018}{{16}{2018{a}}{{Duchi \& Ruan}}{{Duchi and Ruan}}}
\bibcite{duchi_stochastic_2018}{{17}{2018{b}}{{Duchi \& Ruan}}{{Duchi and Ruan}}}
\bibcite{Eckstein1989}{{18}{1989}{{Eckstein}}{{}}}
\bibcite{hundal_alternating_2004}{{19}{2004}{{Hundal}}{{}}}
\bibcite{Jadbabaie2015}{{20}{2015}{{Jadbabaie et~al.}}{{Jadbabaie, Rakhlin, Shahrampour, and Sridharan}}}
\bibcite{Nedic2011}{{21}{2011}{{Koshal et~al.}}{{Koshal, Nedi\'{c}, and Shanbhag}}}
\bibcite{NaLi2020}{{22}{2020}{{Li et~al.}}{{Li, Qu, and Li}}}
\bibcite{Lim2012}{{23}{2012}{{Lim \& Glynn}}{{Lim and Glynn}}}
\bibcite{lushchakova_geometric_2020}{{24}{2020}{{Lushchakova}}{{}}}
\bibcite{pmlr-v119-mai20a}{{25}{2020{a}}{{Mai \& Johansson}}{{Mai and Johansson}}}
\bibcite{pmlr-v119-mai20b}{{26}{2020{b}}{{Mai \& Johansson}}{{Mai and Johansson}}}
\bibcite{Mazumder2019}{{27}{2019}{{Mazumder et~al.}}{{Mazumder, Choudhury, Iyengar, and Sen}}}
\bibcite{Nghiem2018}{{28}{2018}{{Nghiem et~al.}}{{Nghiem, Stathopoulos, and Jones}}}
\bibcite{Ongie2020}{{29}{2020}{{Ongie et~al.}}{{Ongie, Jalal, C.A.~Metzler, Dimakis, and Willett}}}
\bibcite{Rockafellar1976}{{30}{1976}{{Rockafellar}}{{}}}
\bibcite{Ryu2015}{{31}{2016}{{Ryu \& Boyd}}{{Ryu and Boyd}}}
\bibcite{ryu_operator_2020}{{32}{2020}{{Ryu et~al.}}{{Ryu, Taylor, Bergeling, and Giselsson}}}
\bibcite{Seijo2011}{{33}{2011}{{Seijo \& Sen}}{{Seijo and Sen}}}
\bibcite{Sherson2018}{{34}{2018}{{Sherson et~al.}}{{Sherson, Heusdens, and Kleijn}}}
\bibcite{Simonetto20XX}{{35}{2017}{{Simonetto}}{{}}}
\bibcite{simonetto_smooth_2021}{{36}{2021}{{Simonetto}}{{}}}
\bibcite{Simonetto2014d}{{37}{2014}{{Simonetto \& Leus}}{{Simonetto and Leus}}}
\bibcite{simonetto_time-varying_2020}{{38}{2020}{{Simonetto et~al.}}{{Simonetto, Dallâ€™Anese, Paternain, Leus, and Giannakis}}}
\bibcite{Taylor2017}{{39}{2017}{{Taylor}}{{}}}
\bibcite{Taylor2016}{{40}{2016}{{Taylor et~al.}}{{Taylor, Hendrickx, and Glineur}}}
\bibcite{valentine_lipschitz_1945}{{41}{1945}{{Valentine}}{{}}}
\bibstyle{icml2021}
\citation{simonetto_smooth_2021}
\citation{reich_projection_2015}
\citation{reich_projection_2015}
\citation{bauschke_convex_2017}
\citation{artacho_new_2018}
\citation{lushchakova_geometric_2020}
\citation{reich_projection_2015,bauschke_convex_2017}
\citation{bauschke_convex_2017}
\citation{reich_projection_2015}
\newlabel{fig:projection_methods}{{4}{10}{Comparison of feasibility and best approximation methods}{figure.4}{}}
\citation{hundal_alternating_2004,bauschke_characterizing_2009}
\newlabel{eq:alternating_projections}{{24}{11}{}{equation.C.24}{}}
\newlabel{fig:map-histogram}{{5}{11}{Histogram of iterations required by MAP over $10000$ repetitions}{figure.5}{}}
\newlabel{fig:map-changing-d}{{6}{11}{Mean number of iterations required by MAP as a function of the number of sets, computed over $10000$ repetitions. The dashed curve depicts a logarithmic interpolation of the curve}{figure.6}{}}
