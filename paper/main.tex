%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm,fixmath}


%-------------------- COMMANDS
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\fix}{fix}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% new math commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\bv}{\mathbold{b}}
\newcommand{\e}{\mathbold{e}}
\newcommand{\p}{\mathbold{p}}
\newcommand{\q}{\mathbold{q}}
\newcommand{\tv}{\mathbold{t}}
\newcommand{\x}{\mathbold{x}}
\newcommand{\y}{\mathbold{y}}
\newcommand{\vv}{\mathbold{v}}
\newcommand{\z}{\mathbold{z}}
\newcommand{\xx}{\pmb{\xi}}

\newcommand{\Am}{\mathbold{A}}
\renewcommand{\Im}{\mathbold{I}}
\newcommand{\Pm}{\mathbold{P}}

\newcommand{\I}{\mathcal{I}}
\newcommand{\T}{\mathcal{T}}

\newcommand{\Ts}{T_\mathrm{s}}

\newcommand{\nicola}[1]{{\color{blue}#1}}


% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{OpReg-Boost}

\begin{document}

\twocolumn[
\icmltitle{OpReg-Boost: Learning to Accelerate Online Algorithms \\ with Operator Regression}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nicola Bastianello}{unipd}
\icmlauthor{Andrea Simonetto}{ibm}
\icmlauthor{Emiliano Dall'Anese}{cu}
\end{icmlauthorlist}

\icmlaffiliation{unipd}{Department of Information Engineering (DEI), University of Padova, Padova, Italy}
\icmlaffiliation{ibm}{IBM Research Europe, Dublin, Ireland}
\icmlaffiliation{cu}{Department of Electrical, Computer, and Energy Engineering (ECEE), University of Colorado Boulder, Boulder, Colorado, USA}

\icmlcorrespondingauthor{Nicola Bastianello}{nicola.bastianello.3@phd.unipd.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{optimization, online optimization, operator theory, regression, acceleration}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this paper we propose a learning-based acceleration scheme for online optimization. .... novel operator regression ...
\end{abstract}


%------------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}

.... problem formulation, literature review, contribution statement .....


%------------------------------------------------------------------------------
\section{Operator Regression}\label{sec:opreg}

%------------------------------------------------
\subsection{Problem formulation}
Consider the following \emph{convex, time-varying} optimization problem
\begin{equation}\label{eq:continuous-time-problem}
	\x^*(t) \in \argmin f(\x; t) + g(\x; t)
\end{equation}
where $f : \R^n \times \R_+ \to \R$ and $g : \R^n \times \R_+ \to \R \cup \{ +\infty \}$ are closed, convex and proper functions, optionally with $g \equiv 0$. In particular, we are interested in solving the following sequence of static problems derived from sampling~\eqref{eq:continuous-time-problem} at the times $\{ t_k \}_{k \in \N}$, $t_{k+1} - t_k = \Ts$:
\begin{equation}\label{eq:base-problem}
	\x^*(t) \in \argmin f(\x; t_k) + g(\x; t_k).
\end{equation}

Problem~\eqref{eq:base-problem} is convex but not strongly so, which means that the performance of online algorithms applied to it is worse than the performance attainable for strongly convex problems. As an example, consider the case $g \equiv 0$, and apply an online gradient descent: if the problem is convex the (static) regret is $O(\sqrt{T})$, with $T$ the number of sampling times, while if the problem is strongly convex the regret is $O(\log(T))$, section~3.1 in \cite{hazan_introduction_2016}. \nicola{Instead of citing Hazan, we should derive a result in an appendix, for example for the fixed point residual}

As we can see from the results just mentioned, in general in online optimization it is not possible to achieve zero regret, due to the dynamic nature of the problem. However, for strongly convex problems smaller regrets can be achieved.

The goal then is to design learning techniques that allow to achieve strongly convex-like performance while tracking a solution trajectory of the problem~\eqref{eq:base-problem} with good accuracy.


%------------------------------------------------
\subsection{Operator regression}
First of all, we define the observations
$$
	\y_i = \T_f \x_i + \e_i, \quad i \in [D]
$$
of the operator we want to regularize. Now, using Fact~2.2 in \cite{ryu_operator_2020} we know that an operator $\T$ is $\zeta$-contractive interpolable if and only if it satisfies
$$
	\norm{\T \x_i - \T \x_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2, \quad \forall i,j \in [D], \ i \neq j.
$$
Therefore, we can define the following constrained regression problem:
\begin{equation}\label{eq:operator-regression}
\begin{split}
	\hat{\tv}_i &= \argmin_{\tv_i \in \R^n} \frac{1}{2} \sum_{i \in [D]} \norm{\tv_i - \y_i}^2 \\
	&\text{s.t.} \ \norm{\tv_i - \tv_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2 \ \forall i,j \in [D], i \neq j
\end{split}
\end{equation}
where again the cost function is a least square terms on the observations and the constraints enforce contractiveness.


\paragraph{Auto-tuning contraction}
We can modify~\eqref{eq:operator-regression} by including $w = \zeta^2$ as an unknown of the regression problem, which becomes
\begin{equation}
\begin{split}
	\hat{\tv}_i &= \argmin_{\tv_i \in \R^n} \frac{1}{2} \sum_{i \in [D]} \norm{\tv_i - \y_i}^2 + \frac{c}{2} w^2 \\
	&\text{s.t.} \ \norm{\tv_i - \tv_j}^2 - w \norm{\x_i - \x_j}^2 \leq 0 \ \forall i,j \in [D], i \neq j
\end{split}
\end{equation}
this way the contraction constant does not need to be specified, and is \emph{auto-tuned} by the regression.


\paragraph{PRS-based solver}
.... brief description of basic characteristics, reference to the appendix ....


%------------------------------------------------------------------------------
\section{OpReg-Boost}\label{sec:online-opreg}

Let now $\T_k : \R^n \to \R^n$ be a solver for the convex problem observed at time $t_k$, and assume that the operator has a term $\T_{f,k}$ depending exclusively on $f$ (\emph{e.g.} we have $\T_k = \T_k' \circ \T_{f,k}$). Since the sampled problem is convex, the solver $\T_k$ is non-expansive (actually, averaged, to guarantee convergence).

The idea then is to try and learn an approximation of $\T_k$ that is \emph{contractive} rather than non-expansive.

Again, w.l.o.g. we are only interested in learning the term $\T_{f,k}$ that depends on $f$. The algorithm can be described as follows: at each time $t_k$ do:
\begin{enumerate}
	\item sample a new problem, that is, observe $f(\x; t_k)$ and $g(\x; t_k)$;

	\item approximate the term $\T_{f,k}$ of the solver with a contractive one, and
	
	\item apply the resulting solver. For example if $\T_k = \T_k' \circ \T_{f,k}$, then we apply $\T_k = \T_k' \circ \hat{\T}_{f,k}$ where $\hat{\T}_{f,k}$ is the learned operator.
\end{enumerate}
The learning step is performed using the (novel) constrained operator regression OpReg described in the following.

In particular we have the following algorithm \nicola{make this pseudo code}
For $\ell \in 0, 1, \ldots, M$ (for some $M \in \N$):
\begin{itemize}
	\item let $\x_k^\ell$ be the current approximate solution, we choose $D-1$ points around it, \emph{e.g.}
	$$
		\x_i = \x_k^\ell + \p_i, \ i = 2, \ldots, D, \ \p_i \sim \mathcal{N}(0, \sigma^2 \Im_n)
	$$
	and set $\x_1 = \x_k^\ell$;
	
	\item we compute the data $\tv_i = \T_{f,k} \x_i$, $i = 1, \ldots, D$, for example $\tv_i = \x_i - \alpha \nabla f(\x_i; t_k)$;
	
	\item we solve the OpReg, and use the approximate operator at $\x_k^\ell$, that is $\tv_1$, in the chosen solver $\T_k$:
	$$
		\x_k^{\ell+1} = \prox_{\alpha g}(\tv_1)
	$$
	where $\alpha$ is a step-size.
\end{itemize}


%------------------------------------------------
\subsection{Interpolated version}





%------------------------------------------------------------------------------
\section{Numerical Results}\label{sec:numerical}


%------------------------------------------------
\subsection{Simulations set-up}
We consider the following time-varying problem:
\begin{equation}\label{eq:tv-problem}
	\x^*(t_k) = \argmin_{\x \in \R^n} \frac{1}{2} \norm{\Am \x - \bv(t_k)}^2 + w \norm{\x}_1
\end{equation}
with $n = 10$, $\Am$ matrix with maximum and minimum (non-zero) eigenvalues $\sqrt{L} = 10^8$, $\sqrt{\mu} = 1$, and with rank $5$; $\y(t_k)$ has sinusoidal components with $3$ zero components. Due to $\Am$ being rank deficient, the cost $f$ is convex but not strongly so.


%------------------------------------------------
\subsection{Results}




%% FIGURE
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
%\caption{Historical locations and number of accepted papers for International
%Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
%Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
%produced, the number of accepted papers for ICML 2008 was unknown and instead
%estimated.}
%\label{icml-historical}
%\end{center}
%\vskip -0.2in
%\end{figure}


%% ALGORITHM
%\begin{algorithm}[tb]
%   \caption{Bubble Sort}
%   \label{alg:example}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} data $x_i$, size $m$
%   \REPEAT
%   \STATE Initialize $noChange = true$.
%   \FOR{$i=1$ {\bfseries to} $m-1$}
%   \IF{$x_i > x_{i+1}$}
%   \STATE Swap $x_i$ and $x_{i+1}$
%   \STATE $noChange = false$
%   \ENDIF
%   \ENDFOR
%   \UNTIL{$noChange$ is $true$}
%\end{algorithmic}
%\end{algorithm}

%% TABLE
%\begin{table}[t]
%\caption{Classification accuracies for naive Bayes and flexible
%Bayes on various data sets.}
%\label{sample-table}
%\vskip 0.15in
%\begin{center}
%\begin{small}
%\begin{sc}
%\begin{tabular}{lcccr}
%\toprule
%Data set & Naive & Flexible & Better? \\
%\midrule
%Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
%Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
%Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
%Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
%Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
%Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
%Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
%Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
%\bottomrule
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}




\bibliography{references}
\bibliographystyle{icml2021}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%------------------------------------------------------------------------------
\section{PRS-Based QCQP solver}
In this section we present a solver for OpReg which can be efficiently parallelized, inspired by the approach in \cite{simonetto_smooth_2021}.

The idea is as follows: each pair of data points $i, j \in [D]$, $i \neq j$, gives rise to one constraint, for a total of $D(D-1)/2$ constraints. We define the following set of pairs
$$
	\mathcal{V} = \left\{ e = (i,j) \ | \ i, j \in [D], \ i < j \right\}
$$
which are ordered (that is, for example we take $(1,2)$ and not $(2,1)$, to avoid counting constraints twice). Clearly to each pair $e = (i,j)$ corresponds the constraint $\norm{\tv_i - \tv_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2$.

Let now $\tv_{i,e}$ and $\tv_{j,e}$ be copies of $\tv_i$ and $\tv_j$ associated to the $e$-th constraint; then we can equivalently reformulate the OpReg~\eqref{eq:operator-regression} as
\begin{subequations}\label{eq:equivalent-problem}
\begin{align}
	&\min_{\tv_{i,e}, \tv_{j,e}} \frac{1}{2 (D-1)} \sum_{e \in \mathcal{V}} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 \\
	&\text{s.t.} \ \norm{\tv_{i,e} - \tv_{j,e}}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2 \label{eq:interpolation-constraints} \\
	&\qquad \tv_{i,e} = \tv_{i,e'} \ \forall e, e' | i \sim e, e'. \label{eq:consensus-constraints}
\end{align}
\end{subequations}
Clearly~\eqref{eq:equivalent-problem} is a strongly convex problem with convex constraints defined in the variables $\tv_{i,e}$.

%----------------------------------------
\subsection{PRS solver}
Let $\xx$ be the vector stacking all the $\tv_{i,e}$, then the problem is equivalent to
\begin{align*}
	\min_{\xx} f(\xx) + g(\xx)
\end{align*}
where
$$
	f(\xx) = \frac{1}{2(D-1)} \norm{\xx - \y}^2 + f_1(\xx)
$$
with $f_1$ the indicator function imposing~\eqref{eq:interpolation-constraints} and $g$ the indicator function imposing the ``consensus'' constraints~\eqref{eq:consensus-constraints}. The problem can then be solved using the Peaceman-Rachford splitting (PRS) characterized by the following updates $\ell \in \mathbb{N}$:
\begin{subequations}
\begin{align}
	&\xx^\ell = \prox_{\rho f}(\z^\ell) \label{eq:prs-xi} \\
	&\vv^\ell = \prox_{\rho g}(2 \xx^\ell - \z^\ell) \\
	&\z^{\ell+1} = \z^\ell + \vv^\ell - \xx^\ell.
\end{align}
\end{subequations}
The proximal of $g$ corresponds to the projection onto the consensus space, and thus can be characterized simply by
$$
	\vv_{i,e}^\ell = \frac{1}{D-1} \sum_{e' | i \sim e'} \left( 2 \tv_{i,e'}^\ell - \z_{e'}^\ell\right).
$$
Regarding the proximal of $f$, is is clear that $f$ is separable, in the sense that it can be written as
$$
	f(\xx) = \sum_{e \in \mathcal{V}} \left[ \frac{1}{2 (D-1)} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 + \iota_{e}(\tv_{i,e},\tv_{j,e}) \right]
$$
where $\iota_{e}$ denotes the indicator function of~\eqref{eq:interpolation-constraints}. Therefore, the update~\eqref{eq:prs-xi} can be solved by solving (possibly in parallel) the problems
\begin{equation}\label{eq:local-updates}
\begin{split}
	(\tv_{i,e},\tv_{j,e}) &= \argmin \left\{ \frac{1}{2 (D-1)} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 + \frac{1}{2\rho} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \z_e^\ell} \right\}\\
	&\text{s.t.} \quad \norm{\tv_{i,e} - \tv_{j,e}}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2.
\end{split}
\end{equation}

%------------------------------------------------------------
\subsection{Local updates}
The problems~\eqref{eq:local-updates} are quadratic programs with quadratic constraints, that is, they can be written in the form
\begin{subequations}
\begin{align}
	&\min_{\xx} \frac{1}{2} \xx^\top \Pm_0 \xx + \langle \q_0, \xx \rangle \\
	&\text{s.t.} \ \frac{1}{2} \xx^\top \Pm_1 \xx + \langle \q_1, \xx \rangle + r_1 \leq 0.
\end{align}
\end{subequations}
In particular, for the cost function we have
$$
	\Pm_0 = \left( \frac{1}{D-1} + \frac{1}{\rho} \right) \Im_{2n}, \quad \q_0 = - \left( \frac{1}{D-1} \begin{bmatrix} \y_i \\ \y_j \end{bmatrix} + \frac{1}{\rho} \z_e^\ell \right)
$$
and for the constraint
$$
	\Pm_1 = 2 \begin{bmatrix} \Im_n & - \Im_n \\ - \Im_n & \Im_n \end{bmatrix}, \quad \q_1 = 0{2n}, \quad r_1 = - \zeta^2 \norm{\x_i - \x_j}^2.
$$

%------------------------------------------------------------------------------
\section{Interpolation Using MAP}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.