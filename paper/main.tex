%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm,fixmath}


%-------------------- COMMANDS
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\fix}{fix}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% new math commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\f}{\mathbold{f}}
\newcommand{\bdelta}{\mathbold{\delta}}
\newcommand{\transp}{\top}
\newcommand{\bv}{\mathbold{b}}
\newcommand{\e}{\mathbold{e}}
\newcommand{\p}{\mathbold{p}}
\newcommand{\q}{\mathbold{q}}
\newcommand{\tv}{\mathbold{t}}
\newcommand{\x}{\mathbold{x}}
\newcommand{\y}{\mathbold{y}}
\newcommand{\vv}{\mathbold{v}}
\newcommand{\z}{\mathbold{z}}
\newcommand{\xx}{\pmb{\xi}}

\newcommand{\Am}{\mathbold{A}}
\renewcommand{\Im}{\mathbold{I}}
\newcommand{\Pm}{\mathbold{P}}

\newcommand{\I}{\mathcal{I}}
\newcommand{\T}{\mathcal{T}}

\newcommand{\Ts}{T_\mathrm{s}}

\newcommand{\nicola}[1]{{\color{blue}#1}}


% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{OpReg-Boost}

\begin{document}

\twocolumn[
\icmltitle{OpReg-Boost: Learning to Accelerate Online Algorithms \\ with Operator Regression}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nicola Bastianello}{unipd}
\icmlauthor{Andrea Simonetto}{ibm}
\icmlauthor{Emiliano Dall'Anese}{cu}
\end{icmlauthorlist}

\icmlaffiliation{unipd}{Department of Information Engineering (DEI), University of Padova, Padova, Italy}
\icmlaffiliation{ibm}{IBM Research Europe, Dublin, Ireland}
\icmlaffiliation{cu}{Department of Electrical, Computer, and Energy Engineering (ECEE), University of Colorado Boulder, Boulder, Colorado, USA}

\icmlcorrespondingauthor{Nicola Bastianello}{nicola.bastianello.3@phd.unipd.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{optimization, online optimization, operator theory, regression, acceleration}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We present OpReg-Boost, a novel acceleration scheme to boost the convergence properties and lessen the asymptotical error of online algorithms for time-varying (weakly) convex optimization problems. OpReg-Boost is built to learn the closest algorithm to a given online algorithm that has all the algorithmic properties one needs for fast convergence, and it is based on the concept of operator regression. We show how to build OpReg-Boost by using a Peaceman-Rachford solver, and further boost [... interpolation-MAP..]. Simulation results showcase the [better/increase/..] properties of OpReg-Boost w.r.t. the more classical [...], and its close relative convex-regression-boost which is also novel but significantly less performing.  
\end{abstract}


%------------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}

In recent years, we have witnessed the growing interest in time-varying optimization problems, where the cost function, constraints, or both are parametrized over data streams, and therefore are changing over time~\cite{a bunch}. Notable applications for the machine learning community are subspace tracking for video streaming and online sparse subspace clustering, see~\cite{1,2,3} and references therein. 

Solving time-varying optimization problems with online algorithms amounts to find and track continuously changing optimizers. If we let the optimizers trajectory be defined as $\x^*(t)$, a notable results for online algorithms in this context is that if the \emph{path-length} $\sum_{k=1}^T \|\x^*(t_{k+1}) - \x^*(t_{k})\|$, for subsequent time instances $t_{k+1}, t_k$, is not sub-linear in $T$, then online algorithms will have an asymptotical error $O(1)$~\cite{.,.} (or $O(T)$ in terms of pertinent notions of regret)\footnote{Jad.. }. While errors can be made small, for instance with the use of prediction-correction algorithms~\cite{.,.}, the existence of an asymptotical error remains a distinctive feature of online algorithms for time-varying optimization. 

An eye-opening intuition is then to use the existence of said error to one's advantage, by introducing regularizations in the problem formulation that boost the algorithms convergence. In time-varying scenarios, where convergence rate is crucial, often time then one obtains smaller asymptotical error w.r.t. the original problem if one uses regularizations than if they do not. To reiterate, surprisingly, often \emph{there is no trade-off between accuracy and speed} and regularized problems offer better speed and asymptotical errors w.r.t. the original problem, even though we are modifying the problem formulation~\cite{.,.}.  

By building on this field-defining fact, once can ask what is the best regularization for a time-varying problem at hand? In this paper, we explore this question in two slightly different angles. First, we ask ourselves: 

{\bf (Q1) } \emph{what is the closest problem to a given time-varying problem that has all the functional properties we need for fast convergence?}

This gives rise to convex-regression-based boosting. 
 
To fix the ideas, let us consider Figure.. where we have depicted a .. (as a bonus, since we define ..) 

\begin{figure}
\centering
\includegraphics[width=.925\columnwidth]{Figures/explanation}
\caption{...}
\label{fig.1}
\end{figure}

Convex regression is however not the only way to learn good problems, and not the best one (as we are going to explore in the simulation section). A better way is operator regression, which stems from the second question we ask: 

{\bf (Q2) } \emph{what is the closest algorithm to a given online algorithm for solving a time-varying problem that has all the algorithmic properties we need for fast convergence?  
}






Explain OpReg.. 

In this paper, we offer the following contributions,
\begin{enumerate}
\item . 
\end{enumerate}

\subsection{Literature review}

Time-varying optimization and online algorithms are widely treated in~\cite{.,.,.}, while a more machine learning approach is offered in~\cite{.} and subsequent work. The notion that regularizations help convergence possibly without introducing extra asymptotical error is presented in the papers~\cite{.,.,.}. While we refer to~\cite{1,2} for seminal papers close in spirit in the static domain. 

Learning the optimize and regularize is a growing research topic, and we cite~\cite{.,.}, as related work, even though not on the problem we are interested in solving here. 

Convex regression is treated extensively in~\cite{.,.,.}, while recently being generalized to smooth strongly convex functions~\cite{.} based on A. Taylor's work~\cite{.}.

Operator regression is a recent and at the same old topic. We are going to build on the recent work~\cite{.} and the 19?? paper~\cite{.}.

The acceleration schemes that we compare with are .. 

\subsection{Notation}

Notation is wherever possible standard. We use the concept of $\mu$-weakly convex function, to indicate a function $f:\R^n \to \R$ that becomes convex by adding the term $\frac{\mu}{2} \|\x-\x_0\|^2_2$, $\mu >0$ (see~\cite{1,2,3}). The set of convex functions on $\R^n$ that are $L$-smooth (i.e., have $L$-Lipschitz continuous gradient) and $m$-strongly convex is indicated with $\mathcal{S}_{m,L}(\R^n)$.

An operator $\T: \R^n \to \R^n$ is said to be non-expansive iff $\|T \x - T \y \| \leq \|\x- \y\|$, for all $\x, \y \in \R^n$, whereas it is $\zeta$-contractive, $\zeta\in(0,1)$, iff $\|T \x - T \y \| \leq \zeta \|\x- \y\|$, for all $\x, \y \in \R^n$.


\section{Problem Formulation}

We are interested in solving the following time-varying optimization problem,
\begin{equation}\label{eq:continuous-time-problem}
	\x^*(t) \in \argmin_{\x} f(\x; t) + g(\x; t)
\end{equation}
where $f : \R^n \times \R_+ \to \R$ is closed, proper, and $\mu$-weakly convex, whereas $g : \R^n \times \R_+ \to \R \cup \{ +\infty \}$ is closed, convex and proper function, optionally with $g \equiv 0$. Upon sampling the problem at discrete time instances $t_k$, we then obtain the sequence of time-invariant problems,
\begin{equation}\label{eq:base-problem}
	\x^*(t_k) \in \argmin_{\x} f(\x; t_k) + g(\x; t_k), \qquad k \in \N. 
\end{equation}
Our overarching goal is to set up an online algorithm $\mathcal{A}$ that generate a sequence of approximate optimizers $\{\x_k\}_{k\in \N}$, such that the asymptotical tracking error 
\begin{equation}
\limsup_{k \to \infty} \|\x_k - \x^*(t_k)\|,
\end{equation}
is as small as possible. Our main blanket assumption is that optimizers\footnote{..} at subsequent time instances cannot be arbitrarily far apart, that is, 
\begin{equation}
\|\x^*(t_{k+1}) - \x^*(t_k)\| \leq \Delta < +\infty, \qquad k \in \N,
\end{equation}
which in turn guarantees that the path-length grows linearly in time, 
\begin{equation}
\sum_{k\in \N} \|\x^*(t_{k+1}) - \x^*(t_k)\| \leq \Delta k.
\end{equation}

The first important and known result here is that if function $f$ is in $\mathcal{S}_{m,L}(\R^n)$ uniformly in $k$, then an online algorithm $A$ (here a forward-backward algorithm) can obtain linear convergence to the asymptotical error bound $\Delta/(1-\gamma)$, with $\gamma = ..$; much weaker results hold for the general (weakly) convex case~\cite{.}.

The second important result is that a Tikhonov regularization $..$..

%% say something more, change?

With this in place, our two main research questions are
\begin{itemize}
\item {\bf Q1.} Can we project the weakly convex function $f$ onto $\mathcal{S}_{m,L}(\R^n)$

\item {\bf Q1.} Can we project the $A$ onto ??

\end{itemize}

\section{Q1. Convex Regression}

We briefly introduce here the concept of convex regression, while we leave the main technical details to~\cite{1,2,3}. Suppose one has collected $\ell$ noisy measurements of a convex function $\varphi(\x): \R^n \to \R$ (say $y_i$) at points $\x_i \in \R^n$, $i \in I_{\ell}$. Then convex regression is a least-squares approach to estimate the generating function based on the measurements. Formally, letting $\varphi \in \mathcal{S}_{m,L}(\R^n)$, then one would like to solve the infinite-dimensional problem, 
\begin{equation}\label{eq.inf}
\hat{\varphi}_{\ell} \in \argmin_{\psi \in \mathcal{S}_{m, L}(\R^n)}\Big\{ \sum_{i\in I_{\ell}} (y_i - \psi(\x_i))^2 \Big\} \,.
\end{equation}

The problem can be then equivalently decomposed into an estimation on the data points, to find the true function values and gradients at the data point ($\f = [\varphi_i]_{i \in I_{\ell}}$, $\bdelta = [\nabla \varphi_i]_{i \in I_{\ell}}$) which turns out to be a convex quadratically constrained quadratic program, 
\begin{subequations}\label{socp}
\begin{eqnarray}
\minimize_{\f\in\R^\ell\!,~ \bdelta \in\R^{n\ell}} && \sum_{i\in I_{\ell}} (y_i - \varphi_i)^2 \\
\mathrm{subject~to:} &&
\varphi_i - \varphi_j - \bdelta_j^\transp (\x_i - \x_j) \geq \\ && \quad \frac{1}{2(1 - m/L)}\left(\frac{1}{L}\|\bdelta_i - \bdelta_j \|^2_2 + m \|\x_i - \x_j\|_2^2 \right. \nonumber \\ && \quad \left. - 2 \frac{m}{L} (\bdelta_j-\bdelta_i)^\transp (\x_j - \x_i) \right), \quad \forall i, j \in I_n. \nonumber
\end{eqnarray} 
\end{subequations}

And an interpolation scheme, that extends the point estimate over the whole space maintaining the functional properties, 
\begin{equation}\label{interp}
\hat{\varphi}_{\ell}(\x) = \mathrm{conv}(p_i(\x)) + \frac{m}{2} \|\x\|^2_2 \in \mathcal{S}_{m,L}(\R^n),
\end{equation}
where
\begin{multline}
p_i(\x) := \frac{L-m}{2} \| {\x} - \x_i\|_2^2 + (\bdelta_i^*-m \x_i)^\transp \x + \\ - \bdelta_i^{*,\transp} \x_i + f_i^* + m/2\|\x_i\|_2^2,
\end{multline}
and where $\mathrm{conv}(\cdot)$ indicates the convex hull.  

Then, to solve Q1, one could consider a number of evaluations of the function $f(\x; t_k)$ as noisy measurements of an underlying convex function $\varphi_k \in ..$ and using the least-squares approach above to \emph{project} function $f(\cdot; t_k)$ onto the space of ``good'' functions .. 

%% Complete.. put the details in the appendix.. 


%------------------------------------------------------------------------------
\section{Q2. Operator Regression}\label{sec:opreg}

A slightly different approach to convex regression is to look at the algorithm directly, instead of the function. First of all, we remind the reader that any algorithm can be seen as an operator that maps the current approximate optimizer $\x_k$ into a new approximate optimizer $\x_{k+1}$. For example, a gradient algorithm of the form,
\begin{equation}
\x_{k+1} = \x_k - \alpha \nabla_{\x} f(\x_k; t_k) \equiv \underbrace{(I - \alpha \nabla_{x}f_k)}_{=:\T_k} (\x_k)
\end{equation}
where $\T_k: \R^n \to \R^n$ is the gradient algorithm operator. Interpreting algorithms as operators (possibly averaged, monotone, etc.) has been extremely fruitful for characterizing their convergence properties~\cite{,.,tom-also}. 

The counterpart of a smooth strongly convex function $\varphi_k$ in convex regression, is an operator that is contracting, i.e., .. In fact, while smooth strongly convex functions give rise to ...-contracting gradient algorithm operators, the space of contracting operators is larger (i.e., a function $f$ that .. could give rise to ..). 

The key idea now is to interpret .. as measurement of a true .. operator that we want to estimate. 
 
% GENERAL PROBLEM.. 

% FIrst, on the data points

First of all, we define the observations
$$
	\y_i = \T_f \x_i + \e_i, \quad i \in [D]
$$
of the operator we want to regularize. Now, using Fact~2.2 in \cite{ryu_operator_2020} we know that an operator $\T$ is $\zeta$-contractive interpolable if and only if it satisfies
$$
	\norm{\T \x_i - \T \x_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2, \quad \forall i,j \in [D], \ i \neq j.
$$
Therefore, we can define the following constrained regression problem:
\begin{equation}\label{eq:operator-regression}
\begin{split}
	\hat{\tv}_i &= \argmin_{\tv_i \in \R^n} \frac{1}{2} \sum_{i \in [D]} \norm{\tv_i - \y_i}^2 \\
	&\text{s.t.} \ \norm{\tv_i - \tv_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2 \ \forall i,j \in [D], i \neq j
\end{split}
\end{equation}
where again the cost function is a least square terms on the observations and the constraints enforce contractiveness.

%% Then Interpolation here.. 


\subsection{Auto-tuning }

\subsection{PRS-based solver}

\section{OpReg-Boost}

..




To iron this.. 



%------------------------------------------------
\subsection{Problem formulation}
Consider the following \emph{convex, time-varying} optimization problem
\begin{equation}\label{eq:continuous-time-problem}
	\x^*(t) \in \argmin f(\x; t) + g(\x; t)
\end{equation}
where $f : \R^n \times \R_+ \to \R$ and $g : \R^n \times \R_+ \to \R \cup \{ +\infty \}$ are closed, convex and proper functions, optionally with $g \equiv 0$. In particular, we are interested in solving the following sequence of static problems derived from sampling~\eqref{eq:continuous-time-problem} at the times $\{ t_k \}_{k \in \N}$, $t_{k+1} - t_k = \Ts$:
\begin{equation}\label{eq:base-problem}
	\x^*(t) \in \argmin f(\x; t_k) + g(\x; t_k).
\end{equation}


%% PUT THIS SOMEWHERE
Problem~\eqref{eq:base-problem} is convex but not strongly so, which means that the performance of online algorithms applied to it is worse than the performance attainable for strongly convex problems. As an example, consider the case $g \equiv 0$, and apply an online gradient descent: if the problem is convex the (static) regret is $O(\sqrt{T})$, with $T$ the number of sampling times, while if the problem is strongly convex the regret is $O(\log(T))$, section~3.1 in \cite{hazan_introduction_2016}. \nicola{Instead of citing Hazan, we should derive a result in an appendix, for example for the fixed point residual}

As we can see from the results just mentioned, in general in online optimization it is not possible to achieve zero regret, due to the dynamic nature of the problem. However, for strongly convex problems smaller regrets can be achieved.

The goal then is to design learning techniques that allow to achieve strongly convex-like performance while tracking a solution trajectory of the problem~\eqref{eq:base-problem} with good accuracy.


%------------------------------------------------
\subsection{Q2. Operator regression}
First of all, we define the observations
$$
	\y_i = \T_f \x_i + \e_i, \quad i \in [D]
$$
of the operator we want to regularize. Now, using Fact~2.2 in \cite{ryu_operator_2020} we know that an operator $\T$ is $\zeta$-contractive interpolable if and only if it satisfies
$$
	\norm{\T \x_i - \T \x_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2, \quad \forall i,j \in [D], \ i \neq j.
$$
Therefore, we can define the following constrained regression problem:
\begin{equation}\label{eq:operator-regression}
\begin{split}
	\hat{\tv}_i &= \argmin_{\tv_i \in \R^n} \frac{1}{2} \sum_{i \in [D]} \norm{\tv_i - \y_i}^2 \\
	&\text{s.t.} \ \norm{\tv_i - \tv_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2 \ \forall i,j \in [D], i \neq j
\end{split}
\end{equation}
where again the cost function is a least square terms on the observations and the constraints enforce contractiveness.


\paragraph{Auto-tuning contraction}
We can modify~\eqref{eq:operator-regression} by including $w = \zeta^2$ as an unknown of the regression problem, which becomes
\begin{equation}
\begin{split}
	\hat{\tv}_i &= \argmin_{\tv_i \in \R^n} \frac{1}{2} \sum_{i \in [D]} \norm{\tv_i - \y_i}^2 + \frac{c}{2} w^2 \\
	&\text{s.t.} \ \norm{\tv_i - \tv_j}^2 - w \norm{\x_i - \x_j}^2 \leq 0 \ \forall i,j \in [D], i \neq j
\end{split}
\end{equation}
this way the contraction constant does not need to be specified, and is \emph{auto-tuned} by the regression.


\paragraph{PRS-based solver}
.... brief description of basic characteristics, reference to the appendix ....


%------------------------------------------------------------------------------
\section{OpReg-Boost}\label{sec:online-opreg}

Let now $\T_k : \R^n \to \R^n$ be a solver for the convex problem observed at time $t_k$, and assume that the operator has a term $\T_{f,k}$ depending exclusively on $f$ (\emph{e.g.} we have $\T_k = \T_k' \circ \T_{f,k}$). Since the sampled problem is convex, the solver $\T_k$ is non-expansive (actually, averaged, to guarantee convergence).

The idea then is to try and learn an approximation of $\T_k$ that is \emph{contractive} rather than non-expansive.

Again, w.l.o.g. we are only interested in learning the term $\T_{f,k}$ that depends on $f$. The algorithm can be described as follows: at each time $t_k$ do:
\begin{enumerate}
	\item sample a new problem, that is, observe $f(\x; t_k)$ and $g(\x; t_k)$;

	\item approximate the term $\T_{f,k}$ of the solver with a contractive one, and
	
	\item apply the resulting solver. For example if $\T_k = \T_k' \circ \T_{f,k}$, then we apply $\T_k = \T_k' \circ \hat{\T}_{f,k}$ where $\hat{\T}_{f,k}$ is the learned operator.
\end{enumerate}
The learning step is performed using the (novel) constrained operator regression OpReg described in the following.

In particular we have the following algorithm \nicola{make this pseudo code}
For $\ell \in 0, 1, \ldots, M$ (for some $M \in \N$):
\begin{itemize}
	\item let $\x_k^\ell$ be the current approximate solution, we choose $D-1$ points around it, \emph{e.g.}
	$$
		\x_i = \x_k^\ell + \p_i, \ i = 2, \ldots, D, \ \p_i \sim \mathcal{N}(0, \sigma^2 \Im_n)
	$$
	and set $\x_1 = \x_k^\ell$;
	
	\item we compute the data $\tv_i = \T_{f,k} \x_i$, $i = 1, \ldots, D$, for example $\tv_i = \x_i - \alpha \nabla f(\x_i; t_k)$;
	
	\item we solve the OpReg, and use the approximate operator at $\x_k^\ell$, that is $\tv_1$, in the chosen solver $\T_k$:
	$$
		\x_k^{\ell+1} = \prox_{\alpha g}(\tv_1)
	$$
	where $\alpha$ is a step-size.
\end{itemize}


%------------------------------------------------
\subsection{Interpolated version}





%------------------------------------------------------------------------------
\section{Numerical Results}\label{sec:numerical}


%------------------------------------------------
\subsection{Simulations set-up}
We consider the following time-varying problem:
\begin{equation}\label{eq:tv-problem}
	\x^*(t_k) = \argmin_{\x \in \R^n} \frac{1}{2} \norm{\Am \x - \bv(t_k)}^2 + w \norm{\x}_1
\end{equation}
with $n = 10$, $\Am$ matrix with maximum and minimum (non-zero) eigenvalues $\sqrt{L} = 10^8$, $\sqrt{\mu} = 1$, and with rank $5$; $\y(t_k)$ has sinusoidal components with $3$ zero components. Due to $\Am$ being rank deficient, the cost $f$ is convex but not strongly so.


%------------------------------------------------
\subsection{Results}




%% FIGURE
%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
%\caption{Historical locations and number of accepted papers for International
%Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
%Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
%produced, the number of accepted papers for ICML 2008 was unknown and instead
%estimated.}
%\label{icml-historical}
%\end{center}
%\vskip -0.2in
%\end{figure}


%% ALGORITHM
%\begin{algorithm}[tb]
%   \caption{Bubble Sort}
%   \label{alg:example}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} data $x_i$, size $m$
%   \REPEAT
%   \STATE Initialize $noChange = true$.
%   \FOR{$i=1$ {\bfseries to} $m-1$}
%   \IF{$x_i > x_{i+1}$}
%   \STATE Swap $x_i$ and $x_{i+1}$
%   \STATE $noChange = false$
%   \ENDIF
%   \ENDFOR
%   \UNTIL{$noChange$ is $true$}
%\end{algorithmic}
%\end{algorithm}

%% TABLE
%\begin{table}[t]
%\caption{Classification accuracies for naive Bayes and flexible
%Bayes on various data sets.}
%\label{sample-table}
%\vskip 0.15in
%\begin{center}
%\begin{small}
%\begin{sc}
%\begin{tabular}{lcccr}
%\toprule
%Data set & Naive & Flexible & Better? \\
%\midrule
%Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
%Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
%Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
%Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
%Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
%Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
%Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
%Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
%\bottomrule
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}




\bibliography{references}
\bibliographystyle{icml2021}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%------------------------------------------------------------------------------
\section{PRS-Based QCQP solver}
In this section we present a solver for OpReg which can be efficiently parallelized, inspired by the approach in \cite{simonetto_smooth_2021}.

The idea is as follows: each pair of data points $i, j \in [D]$, $i \neq j$, gives rise to one constraint, for a total of $D(D-1)/2$ constraints. We define the following set of pairs
$$
	\mathcal{V} = \left\{ e = (i,j) \ | \ i, j \in [D], \ i < j \right\}
$$
which are ordered (that is, for example we take $(1,2)$ and not $(2,1)$, to avoid counting constraints twice). Clearly to each pair $e = (i,j)$ corresponds the constraint $\norm{\tv_i - \tv_j}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2$.

Let now $\tv_{i,e}$ and $\tv_{j,e}$ be copies of $\tv_i$ and $\tv_j$ associated to the $e$-th constraint; then we can equivalently reformulate the OpReg~\eqref{eq:operator-regression} as
\begin{subequations}\label{eq:equivalent-problem}
\begin{align}
	&\min_{\tv_{i,e}, \tv_{j,e}} \frac{1}{2 (D-1)} \sum_{e \in \mathcal{V}} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 \\
	&\text{s.t.} \ \norm{\tv_{i,e} - \tv_{j,e}}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2 \label{eq:interpolation-constraints} \\
	&\qquad \tv_{i,e} = \tv_{i,e'} \ \forall e, e' | i \sim e, e'. \label{eq:consensus-constraints}
\end{align}
\end{subequations}
Clearly~\eqref{eq:equivalent-problem} is a strongly convex problem with convex constraints defined in the variables $\tv_{i,e}$.

%----------------------------------------
\subsection{PRS solver}
Let $\xx$ be the vector stacking all the $\tv_{i,e}$, then the problem is equivalent to
\begin{align*}
	\min_{\xx} f(\xx) + g(\xx)
\end{align*}
where
$$
	f(\xx) = \frac{1}{2(D-1)} \norm{\xx - \y}^2 + f_1(\xx)
$$
with $f_1$ the indicator function imposing~\eqref{eq:interpolation-constraints} and $g$ the indicator function imposing the ``consensus'' constraints~\eqref{eq:consensus-constraints}. The problem can then be solved using the Peaceman-Rachford splitting (PRS) characterized by the following updates $\ell \in \mathbb{N}$:
\begin{subequations}
\begin{align}
	&\xx^\ell = \prox_{\rho f}(\z^\ell) \label{eq:prs-xi} \\
	&\vv^\ell = \prox_{\rho g}(2 \xx^\ell - \z^\ell) \\
	&\z^{\ell+1} = \z^\ell + \vv^\ell - \xx^\ell.
\end{align}
\end{subequations}
The proximal of $g$ corresponds to the projection onto the consensus space, and thus can be characterized simply by
$$
	\vv_{i,e}^\ell = \frac{1}{D-1} \sum_{e' | i \sim e'} \left( 2 \tv_{i,e'}^\ell - \z_{e'}^\ell\right).
$$
Regarding the proximal of $f$, is is clear that $f$ is separable, in the sense that it can be written as
$$
	f(\xx) = \sum_{e \in \mathcal{V}} \left[ \frac{1}{2 (D-1)} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 + \iota_{e}(\tv_{i,e},\tv_{j,e}) \right]
$$
where $\iota_{e}$ denotes the indicator function of~\eqref{eq:interpolation-constraints}. Therefore, the update~\eqref{eq:prs-xi} can be solved by solving (possibly in parallel) the problems
\begin{equation}\label{eq:local-updates}
\begin{split}
	(\tv_{i,e},\tv_{j,e}) &= \argmin \left\{ \frac{1}{2 (D-1)} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \begin{bmatrix} \y_i \\ \y_j \end{bmatrix}}^2 + \frac{1}{2\rho} \norm{\begin{bmatrix} \tv_{i,e} \\ \tv_{j,e} \end{bmatrix} - \z_e^\ell} \right\}\\
	&\text{s.t.} \quad \norm{\tv_{i,e} - \tv_{j,e}}^2 \leq \zeta^2 \norm{\x_i - \x_j}^2.
\end{split}
\end{equation}

%------------------------------------------------------------
\subsection{Local updates}
The problems~\eqref{eq:local-updates} are quadratic programs with quadratic constraints, that is, they can be written in the form
\begin{subequations}
\begin{align}
	&\min_{\xx} \frac{1}{2} \xx^\top \Pm_0 \xx + \langle \q_0, \xx \rangle \\
	&\text{s.t.} \ \frac{1}{2} \xx^\top \Pm_1 \xx + \langle \q_1, \xx \rangle + r_1 \leq 0.
\end{align}
\end{subequations}
In particular, for the cost function we have
$$
	\Pm_0 = \left( \frac{1}{D-1} + \frac{1}{\rho} \right) \Im_{2n}, \quad \q_0 = - \left( \frac{1}{D-1} \begin{bmatrix} \y_i \\ \y_j \end{bmatrix} + \frac{1}{\rho} \z_e^\ell \right)
$$
and for the constraint
$$
	\Pm_1 = 2 \begin{bmatrix} \Im_n & - \Im_n \\ - \Im_n & \Im_n \end{bmatrix}, \quad \q_1 = 0{2n}, \quad r_1 = - \zeta^2 \norm{\x_i - \x_j}^2.
$$

%------------------------------------------------------------------------------
\section{Interpolation Using MAP}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.